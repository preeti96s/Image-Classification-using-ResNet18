{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWV8ehxUFGli"
      },
      "source": [
        "2nd Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n4ww222FPI-"
      },
      "source": [
        "Part (**a**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4Kufm4JI0jc",
        "outputId": "3a1df45f-0883-49c8-a30d-a83a3a2c3dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f6f1iMw8Tk1",
        "outputId": "bc4d1b5b-d00b-41bb-fd6d-ba10455abaf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "xlwPigIc-G5l"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model\n",
        "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')   #load resnet18 model\n",
        "num_features = model.fc.in_features     #extract fc layers features \n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "model = model.to(device) #move model to device\n",
        "criterion = nn.CrossEntropyLoss()  #(set loss function)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB0W3uFE-QVs",
        "outputId": "95184367-3afa-4f17-b1ea-501ae3115321"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OGw1FrjWFrcc"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "7DhgjCjBFc_9"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/drive/MyDrive/AIP_data/train\"\n",
        "test_dir = \"/content/drive/MyDrive/AIP_data/test\"\n",
        "batchSize = 532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "JnUf5q73-cSj"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.ImageFolder(train_dir, transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize, shuffle=True, num_workers=2)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batchSize, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6F8YO9hAcXg",
        "outputId": "ab082bf3-e368-4222-d05e-50bb6b7788af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 532\n",
            "Test dataset size: 120\n",
            "Class names: ['bear', 'butterfly', 'camel', 'chimp', 'duck', 'elephant']\n"
          ]
        }
      ],
      "source": [
        "print('Train dataset size:', len(train_dataset))\n",
        "print('Test dataset size:', len(test_dataset))\n",
        "class_names = train_dataset.classes\n",
        "print('Class names:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_keok4HktiuY"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ZEhCwR3voJoz"
      },
      "outputs": [],
      "source": [
        "# Saving train features\n",
        "model.eval()\n",
        "train_features = np.zeros(shape=(532, 512, 1, 1))\n",
        "train_labels = np.zeros(shape=(532,1))\n",
        "batch_size = 4\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        train_features[i * batch_size: (i + 1) * batch_size] = outputs.cpu().data.numpy().argmax()\n",
        "        train_labels[i * batch_size: (i + 1) * batch_size] = labels.cpu().data.numpy().argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "mHKgiOibrIII"
      },
      "outputs": [],
      "source": [
        "# Saving test features\n",
        "model.eval()\n",
        "test_features = np.zeros(shape=(120, 512, 1, 1))\n",
        "test_labels = np.zeros(shape=(120,1))\n",
        "batch_size = 4\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        test_features[i * batch_size: (i + 1) * batch_size] = outputs.cpu().data.numpy().argmax()\n",
        "        test_labels[i * batch_size: (i + 1) * batch_size] = labels.cpu().data.numpy().argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "wNd6w2RjFtC0"
      },
      "outputs": [],
      "source": [
        "train_features = GlobalAveragePooling2D()(train_features).numpy()\n",
        "test_features = GlobalAveragePooling2D()(test_features).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swEUwGvjI-Wt",
        "outputId": "1951af07-3186-4ba1-a356-e4bbd7930d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "pred_labels = knn.predict(test_features)\n",
        "\n",
        "# Evaluate the accuracy of the k-NN classifier\n",
        "accuracy = accuracy_score(test_labels, np.ravel(pred_labels,order='C'))\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SYpBRKSYDg9"
      },
      "source": [
        "Part (**b**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "i1-ABYv3vVuh"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model\n",
        "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')   #load resnet18 model\n",
        "num_features = model.fc.in_features     #extract fc layers features \n",
        "model.fc = nn.Linear(num_features, 6) #(num_of_class == 6)\n",
        "model = model.to(device) #move model to device\n",
        "criterion = nn.CrossEntropyLoss()  #(set loss function)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnRVfNzyCtI1",
        "outputId": "50493f61-9ca1-45a0-d878-6c4d55376236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 running\n",
            "[Train #0] Loss: 2.0184 Acc: 12.2180% Time: 5.4965s\n",
            "Epoch 1 running\n",
            "[Train #1] Loss: 1.6922 Acc: 28.5714% Time: 10.9989s\n",
            "Epoch 2 running\n",
            "[Train #2] Loss: 1.4249 Acc: 53.7594% Time: 16.3724s\n",
            "Epoch 3 running\n",
            "[Train #3] Loss: 1.1819 Acc: 69.7368% Time: 21.8138s\n",
            "Epoch 4 running\n",
            "[Train #4] Loss: 0.9007 Acc: 77.2556% Time: 27.1871s\n",
            "Epoch 5 running\n",
            "[Train #5] Loss: 0.6153 Acc: 86.4662% Time: 32.6960s\n",
            "Epoch 6 running\n",
            "[Train #6] Loss: 0.3974 Acc: 94.9248% Time: 38.4307s\n",
            "Epoch 7 running\n",
            "[Train #7] Loss: 0.2762 Acc: 97.5564% Time: 43.7648s\n",
            "Epoch 8 running\n",
            "[Train #8] Loss: 0.2155 Acc: 97.3684% Time: 50.0935s\n",
            "Epoch 9 running\n",
            "[Train #9] Loss: 0.1757 Acc: 97.7444% Time: 55.5115s\n",
            "Epoch 10 running\n",
            "[Train #10] Loss: 0.1404 Acc: 98.1203% Time: 60.9330s\n",
            "Epoch 11 running\n",
            "[Train #11] Loss: 0.1075 Acc: 98.4962% Time: 66.3954s\n",
            "Epoch 12 running\n",
            "[Train #12] Loss: 0.0797 Acc: 98.8722% Time: 72.2273s\n",
            "Epoch 13 running\n",
            "[Train #13] Loss: 0.0586 Acc: 99.4361% Time: 77.6140s\n",
            "Epoch 14 running\n",
            "[Train #14] Loss: 0.0434 Acc: 99.4361% Time: 83.1695s\n",
            "Epoch 15 running\n",
            "[Train #15] Loss: 0.0326 Acc: 99.8120% Time: 88.5973s\n",
            "Epoch 16 running\n",
            "[Train #16] Loss: 0.0248 Acc: 99.8120% Time: 94.0356s\n",
            "Epoch 17 running\n",
            "[Train #17] Loss: 0.0190 Acc: 99.8120% Time: 100.0545s\n",
            "Epoch 18 running\n",
            "[Train #18] Loss: 0.0147 Acc: 100.0000% Time: 105.4305s\n",
            "Epoch 19 running\n",
            "[Train #19] Loss: 0.0114 Acc: 100.0000% Time: 110.8582s\n",
            "Epoch 20 running\n",
            "[Train #20] Loss: 0.0090 Acc: 100.0000% Time: 116.2510s\n",
            "Epoch 21 running\n",
            "[Train #21] Loss: 0.0072 Acc: 100.0000% Time: 121.7477s\n",
            "Epoch 22 running\n",
            "[Train #22] Loss: 0.0059 Acc: 100.0000% Time: 127.2607s\n",
            "Epoch 23 running\n",
            "[Train #23] Loss: 0.0050 Acc: 100.0000% Time: 132.7136s\n",
            "Epoch 24 running\n",
            "[Train #24] Loss: 0.0042 Acc: 100.0000% Time: 138.2114s\n",
            "[Test #24] Loss: 0.1359 Acc: 95.0000% Time: 139.4093s\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 25   #(set no of epochs)\n",
        "start_time = time.time() #(for showing time)\n",
        "for epoch in range(num_epochs): #(loop for every epoch)\n",
        "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
        "    \"\"\" Training Phase \"\"\"\n",
        "    model.train()    #(training model)\n",
        "    running_loss = 0.   #(set loss 0)\n",
        "    running_corrects = 0 \n",
        "    # load a batch data of images\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        # forward inputs and get output\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.cpu()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # get loss value and update the network weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
        "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
        "    \n",
        "    \"\"\" Testing Phase \"\"\"\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.cpu()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_loss = running_loss / len(test_dataset)\n",
        "    epoch_acc = running_corrects / len(test_dataset) * 100.\n",
        "    print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "qbs24TOnEu7f"
      },
      "outputs": [],
      "source": [
        "save_path = 'image-classifier_resnet_18_final_60_last_tr_epochs.pth'\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpt2xZOPKRqm",
        "outputId": "321402ac-73e9-4e01-ddc3-800f0f7864d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"image-classifier_resnet_18_final_60_last_tr_epochs.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVeMYLcRXE2Q",
        "outputId": "1844f6d8-e666-41a7-ee57-9900be04e279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Train #] Loss: 0.0024 Acc: 100.0000% Time: 5.8952s\n"
          ]
        }
      ],
      "source": [
        "# Testing model for train data\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "train_features = np.zeros(shape=(528, 512, 1, 1))\n",
        "train_labels = np.zeros(shape=(528,1))\n",
        "batch_size = 1\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        output = feature_extractor(inputs)\n",
        "        train_features[i * batch_size: (i + 1) * batch_size] = output.cpu().data.numpy().argmax()\n",
        "        train_labels[i * batch_size: (i + 1) * batch_size] = labels.cpu().data.numpy().argmax()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
        "    print('[Train #] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.\n",
        "          format( epoch_loss, epoch_acc, time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQZkwj4BXJyA",
        "outputId": "2926c518-3518-4d3c-d061-450ee9c6263d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Test #] Loss: 0.1359 Acc: 95.0000% Time: 1.2469s\n"
          ]
        }
      ],
      "source": [
        "# Testing model for test data\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "test_features = np.zeros(shape=(120, 512, 1, 1))\n",
        "test_labels = np.zeros(shape=(120,1))\n",
        "batch_size = 1\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        output = feature_extractor(inputs)\n",
        "        test_features[i * batch_size: (i + 1) * batch_size] = outputs.cpu().data.numpy().argmax()\n",
        "        test_labels[i * batch_size: (i + 1) * batch_size] = labels.cpu().data.numpy().argmax()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_loss = running_loss / len(test_dataset)\n",
        "    epoch_acc = running_corrects / len(test_dataset) * 100.\n",
        "    print('[Test #] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.\n",
        "          format( epoch_loss, epoch_acc, time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_akzo5J5gThx"
      },
      "source": [
        "Part (**c**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "5ODSoMZrgemX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogLt3PW-ukjx",
        "outputId": "a9f93c2e-a78f-4d2a-d754-e9da8450376f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 532\n",
            "Test dataset size: 120\n",
            "Class names: ['bear', 'butterfly', 'camel', 'chimp', 'duck', 'elephant']\n",
            "[1,     1] loss: 0.002\n",
            "[2,     1] loss: 0.000\n",
            "[3,     1] loss: 1.126\n",
            "[4,     1] loss: 0.000\n",
            "[5,     1] loss: 0.000\n"
          ]
        }
      ],
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel,self).__init__()\n",
        "    self.layer1 = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(224*224*3,6)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "# Initialize the network\n",
        "model = MyModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Load the training data\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "train_dir = \"/content/drive/MyDrive/AIP_data/train\"\n",
        "test_dir = \"/content/drive/MyDrive/AIP_data/test\"\n",
        "batchSize = 1\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize, shuffle=True, num_workers=2)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batchSize, shuffle=False, num_workers=2)\n",
        "print('Train dataset size:', len(train_dataset))\n",
        "print('Test dataset size:', len(test_dataset))\n",
        "class_names = train_dataset.classes\n",
        "print('Class names:', class_names)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(15):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader, 0):\n",
        "          \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i == 0:    # print once for each epoch\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "noI3qUBsq9fA"
      },
      "outputs": [],
      "source": [
        "save_path = 'image-classifier_cnn_final_10_last_tr_epochs.pth'\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCh5GmlOrH4n",
        "outputId": "4edd3403-83f3-4c5a-8e0c-afd72f9e0425"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"image-classifier_cnn_final_10_last_tr_epochs.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iYCweGzrSd1",
        "outputId": "904e031a-1849-4513-ca97-b28323d3b568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 30.00%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "accuracy = 100 * correct / total  # accuracy\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
